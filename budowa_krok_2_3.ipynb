{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Milestone II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "import string\n",
    "from textstat import textstat\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Comment_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the creator of monopoly would be speechless ab...</td>\n",
       "      <td>negative</td>\n",
       "      <td>creator monopoly speechless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sometimes im just taking my test and the voice...</td>\n",
       "      <td>negative</td>\n",
       "      <td>taking test voice scare loooool coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even when andy isnt trying to be a genius he is</td>\n",
       "      <td>neutral</td>\n",
       "      <td>andy genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its just usual trimming of the fluff when exce...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>usual trimming fluff excess liquidity withdraw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mom wake your ass up for classes me oh nah nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>mom wake as class meeting bed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment  \\\n",
       "0  the creator of monopoly would be speechless ab...  negative   \n",
       "1  sometimes im just taking my test and the voice...  negative   \n",
       "2    even when andy isnt trying to be a genius he is   neutral   \n",
       "3  its just usual trimming of the fluff when exce...   neutral   \n",
       "4  mom wake your ass up for classes me oh nah nah...  negative   \n",
       "\n",
       "                                Comment_preprocessed  \n",
       "0                        creator monopoly speechless  \n",
       "1             taking test voice scare loooool coming  \n",
       "2                                        andy genius  \n",
       "3  usual trimming fluff excess liquidity withdraw...  \n",
       "4                      mom wake as class meeting bed  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"YoutubeCommentsDataSet_Balanced.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ramka df zawiera kolumny Comment, Sentiment oraz Comment_preprocessed. Zbiór ten jest zbalansowany pod względem sentymentów, oczysczony i pozbawiony duplikatów.\n",
    "Comment_preprocessed zawiera komentarze z usuniętymi stopwords. Pozostawiłyśmy kolumnę Comment bez zmian, gdyż będziemy jej potrzebować do stworzenia niektórych cech  istotnych w budowie modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inżynieria cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W kontekście naszych danych stwierdziłyśmy, że przydatne będą następujące cechy:\n",
    "- Sentiment_Score_nltk (ciągły wskaźnik sentymentu komentarza o warotściach [-1, 1])\n",
    "- Comment_length (długość komentarza)\n",
    "- Starts_with_i (czy komentarz zaczyna się od I)\n",
    "- Personal_Pronoun_count (liczba zaimków osobowych, takich jak he, she, i, etc.)\n",
    "- Number_of_phrases (czyli podział komentarza według spójników \"but\", \"and\", \"because\")\n",
    "- Avg_phrase_length (średnia długość frazy)\n",
    "- Number_of_words (liczba słów)\n",
    "- Avg_word_length (średnia długość słów)\n",
    "- Unique_word_ratio (stosunek liczby unikalnych słów do wszystkich słów, im większa tym bardziej rozbudowane słownictwo)\n",
    "- Readibility_Score (wskaźnik czytelności tekstu mierzony za pomocą Flesch Reading Ease)\n",
    "- Negation_Count (ilość negacji w komentarzu, czyli słów, które wprowadzają zaprzeczenie np. \"not\",\"never\",\"nobody\",\"dont\" itp.)\n",
    "- Rare_Word_Count (Liczenie rzadkich słów na podstawie ich częstotliwości w całym zbiorze danych)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmienna Sentiment_Score_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do obliczania Sentiment Score za pomocą NLTK\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def sentiment_score(comment):\n",
    "    return sia.polarity_scores(comment)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmienna Personal_Pronun_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do liczenia liczby zaimków osobistych\n",
    "personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\",\n",
    "                     \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\",\n",
    "                     \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    "                     \"theirs\", \"themselves\"}\n",
    "\n",
    "def count_personal_pronouns(comment):\n",
    "    words = word_tokenize(comment.lower())\n",
    "    return sum(1 for word in words if word in personal_pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmienne Number_of_phrases, Number_of_words, Avg_phrase_length, Avg_word_length, Unique_word_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do analizy złożoności zdań (np. liczba fraz, słów)\n",
    "connectors = [\"and\", \"but\", \"so\", \"because\", \"which\", \"that\", \"then\", \"if\", \"or\"]\n",
    "def sentence_complexity(comment):\n",
    "    words = word_tokenize(comment)\n",
    "    phrases = re.split(r'\\b(?:' + '|'.join(connectors) + r')\\b', comment)\n",
    "\n",
    "    num_words = len(words)\n",
    "    num_phrases = len(phrases)\n",
    "\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    avg_phrase_length = num_words / num_phrases if num_phrases > 0 else 0\n",
    "    unique_word_ratio = len(set(words)) / num_words if num_words > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Number_of_phrases\": num_phrases,\n",
    "        \"Number_of_words\": num_words,\n",
    "        \"Avg_phrase_length\": avg_phrase_length,\n",
    "        \"Avg_word_length\": avg_word_length,\n",
    "        \"Unique_word_ratio\": unique_word_ratio\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmienna Readability_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do obliczania readability score\n",
    "def readability_score(comment):\n",
    "    return textstat.flesch_reading_ease(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmienna Negation_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do liczenia negacji\n",
    "def negation_count(comment):\n",
    "    negations = {\"not\", \"never\", \"no\", \"none\", \"cannot\", \"nothing\",\"dont\",\"nah\",'wont','cant','doesnt','shouldnt',\"shouldn't\",\"doesn't\"\n",
    "                 \"don't\",\"won't\",\"wouldn't\", \"can't\",\"nobody\",'neither','nope',\"ain't\",\"nowhere\",'wouldnt'}\n",
    "    words = word_tokenize(comment.lower())\n",
    "    return sum(1 for word in words if word in negations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zmienna Rare_Word_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do liczenia rzadkich słów\n",
    "def rare_word_count(comment, all_words):\n",
    "    words = word_tokenize(comment.lower())\n",
    "    rare_words = [word for word in words if all_words[word] < 10]  # Możemy dostosować próg (tutaj <10)\n",
    "    return len(rare_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowujemy cechy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do przygotowania cech\n",
    "def prepare_features(df):\n",
    "    # Dodajemy cechy do DataFrame\n",
    "    df['Sentiment_Score_nltk'] = df['Comment'].apply(sentiment_score)\n",
    "    df['Starts_with_i'] = df['Comment'].str.startswith(\"i\")\n",
    "    df['Comment_Length'] = df['Comment'].str.len()\n",
    "    \n",
    "    sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "    df['Sentiment_num'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "    df[\"Personal_Pronoun_count\"] = df[\"Comment\"].apply(count_personal_pronouns)\n",
    "    df[\"Readability_Score\"] = df[\"Comment\"].apply(readability_score)\n",
    "    df[\"Negation_Count\"] = df[\"Comment\"].apply(negation_count)\n",
    "\n",
    "    df_complexity = df[\"Comment\"].apply(lambda x: sentence_complexity(x))\n",
    "    df_complexity = pd.DataFrame(df_complexity.tolist())\n",
    "    df = pd.concat([df, df_complexity], axis=1)\n",
    "\n",
    "    # Liczenie rzadkich słów\n",
    "    all_words = Counter(' '.join(df['Comment']).lower().split())\n",
    "    df['Rare_Word_Count'] = df['Comment'].apply(lambda x: rare_word_count(x, all_words))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budujemy pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_pipeline():\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"text_features\", FunctionTransformer(prepare_features), ['Comment']) \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Tworzenie i zapisanie pipeline\n",
    "pipeline = build_feature_pipeline()\n",
    "\n",
    "with open(\"feature_pipeline.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pipeline, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplikujemy wszystkie features na naszym zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Comment_preprocessed</th>\n",
       "      <th>Sentiment_Score_nltk</th>\n",
       "      <th>Starts_with_i</th>\n",
       "      <th>Comment_Length</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>Personal_Pronoun_count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>Negation_Count</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_of_words</th>\n",
       "      <th>Avg_phrase_length</th>\n",
       "      <th>Avg_word_length</th>\n",
       "      <th>Unique_word_ratio</th>\n",
       "      <th>Rare_Word_Count</th>\n",
       "      <th>Number_of_phrases</th>\n",
       "      <th>Number_of_words</th>\n",
       "      <th>Avg_phrase_length</th>\n",
       "      <th>Avg_word_length</th>\n",
       "      <th>Unique_word_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the creator of monopoly would be speechless ab...</td>\n",
       "      <td>negative</td>\n",
       "      <td>creator monopoly speechless</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.87</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sometimes im just taking my test and the voice...</td>\n",
       "      <td>negative</td>\n",
       "      <td>taking test voice scare loooool coming</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>False</td>\n",
       "      <td>98</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>86.03</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even when andy isnt trying to be a genius he is</td>\n",
       "      <td>neutral</td>\n",
       "      <td>andy genius</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>68.77</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>its just usual trimming of the fluff when exce...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>usual trimming fluff excess liquidity withdraw...</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>True</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>53.21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>4.093023</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>4.093023</td>\n",
       "      <td>0.813953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mom wake your ass up for classes me oh nah nah...</td>\n",
       "      <td>negative</td>\n",
       "      <td>mom wake as class meeting bed</td>\n",
       "      <td>-0.7579</td>\n",
       "      <td>False</td>\n",
       "      <td>88</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>92.46</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.238095</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.238095</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Sentiment  \\\n",
       "0  the creator of monopoly would be speechless ab...  negative   \n",
       "1  sometimes im just taking my test and the voice...  negative   \n",
       "2    even when andy isnt trying to be a genius he is   neutral   \n",
       "3  its just usual trimming of the fluff when exce...   neutral   \n",
       "4  mom wake your ass up for classes me oh nah nah...  negative   \n",
       "\n",
       "                                Comment_preprocessed  Sentiment_Score_nltk  \\\n",
       "0                        creator monopoly speechless                0.0000   \n",
       "1             taking test voice scare loooool coming                0.0258   \n",
       "2                                        andy genius                0.0000   \n",
       "3  usual trimming fluff excess liquidity withdraw...                0.2023   \n",
       "4                      mom wake as class meeting bed               -0.7579   \n",
       "\n",
       "   Starts_with_i  Comment_Length  Sentiment_num  Personal_Pronoun_count  \\\n",
       "0          False              58             -1                       0   \n",
       "1          False              98             -1                       2   \n",
       "2          False              47              0                       1   \n",
       "3           True             218              0                       4   \n",
       "4          False              88             -1                       3   \n",
       "\n",
       "   Readability_Score  Negation_Count  ...  Number_of_words  Avg_phrase_length  \\\n",
       "0              52.87               0  ...               10          10.000000   \n",
       "1              86.03               0  ...               19           6.333333   \n",
       "2              68.77               0  ...               11          11.000000   \n",
       "3              53.21               0  ...               43           8.600000   \n",
       "4              92.46               5  ...               21          21.000000   \n",
       "\n",
       "   Avg_word_length  Unique_word_ratio  Rare_Word_Count  Number_of_phrases  \\\n",
       "0         4.900000           1.000000                0                  1   \n",
       "1         4.210526           0.947368                1                  3   \n",
       "2         3.363636           1.000000                1                  1   \n",
       "3         4.093023           0.813953                9                  5   \n",
       "4         3.238095           0.809524                0                  1   \n",
       "\n",
       "   Number_of_words  Avg_phrase_length  Avg_word_length  Unique_word_ratio  \n",
       "0               10          10.000000         4.900000           1.000000  \n",
       "1               19           6.333333         4.210526           0.947368  \n",
       "2               11          11.000000         3.363636           1.000000  \n",
       "3               43           8.600000         4.093023           0.813953  \n",
       "4               21          21.000000         3.238095           0.809524  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prepare_features(df)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
